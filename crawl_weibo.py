"""
å¾®åšæ•°æ®é‡‡é›†è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
    .venv\Scripts\python crawl_weibo.py --keyword "AI" --limit 100
"""

import asyncio
import json
import os
import sys
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from loguru import logger

from sentiment_analyzer.crawlers.weibo import WeiboConfig, WeiboCrawler
from sentiment_analyzer.storage.sqlite_store import SQLiteStore


# é…ç½®æ—¥å¿—
logger.remove()
logger.add(sys.stderr, level="INFO")
logger.add("logs/crawl_weibo.log", rotation="10 MB", level="DEBUG")


async def crawl_weibo_data(
    keyword: str = "AI",
    limit: int = 100,
    cookies: str = None
):
    """
    çˆ¬å–å¾®åšæ•°æ®
    
    Args:
        keyword: æœç´¢å…³é”®è¯
        limit: é‡‡é›†æ•°é‡é™åˆ¶
        cookies: å¾®åšç™»å½•Cookieï¼ˆå¯é€‰ï¼‰
    """
    # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
    Path("./data").mkdir(exist_ok=True)
    Path("./logs").mkdir(exist_ok=True)
    
    # åˆå§‹åŒ–å­˜å‚¨
    store = SQLiteStore("./data/sentiment.db")
    await store.initialize()
    
    # é…ç½®å¾®åšçˆ¬è™«
    config = WeiboConfig(
        cookies=cookies,
        use_mobile_api=True,
        timeout=30,
        max_retries=3,
        request_delay=(2.0, 5.0),  # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«å°
    )
    
    logger.info(f"å¼€å§‹é‡‡é›†å¾®åšæ•°æ®: å…³é”®è¯='{keyword}', é™åˆ¶={limit}")
    
    crawler = WeiboCrawler(config)
    await crawler.initialize()
    count = 0
    try:
        async for post in crawler.crawl_search(keyword, limit=limit):
            try:
                # ä¿å­˜ç”¨æˆ·
                if post.user:
                    await store.save_user(post.user)
                    logger.debug(f"ä¿å­˜ç”¨æˆ·: {post.user.username}")
                
                # ä¿å­˜å¸–å­
                await store.save_post(post)
                logger.debug(f"ä¿å­˜å¸–å­: {post.content[:50]}...")
                
                count += 1
                if count % 10 == 0:
                    logger.info(f"å·²é‡‡é›† {count}/{limit} æ¡æ•°æ®")
                
            except Exception as e:
                logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
                continue
    finally:
        await crawler.close()
    
    logger.info(f"é‡‡é›†å®Œæˆ! å…±é‡‡é›† {count} æ¡æ•°æ®")
    
    return count


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å¾®åšæ•°æ®é‡‡é›†å·¥å…·")
    parser.add_argument("--keyword", "-k", default="AI", help="æœç´¢å…³é”®è¯")
    parser.add_argument("--limit", "-l", type=int, default=100, help="é‡‡é›†æ•°é‡é™åˆ¶")
    parser.add_argument("--cookies", "-c", default=None, help="å¾®åšCookieå­—ç¬¦ä¸²")
    
    args = parser.parse_args()
    
    # è¿è¡Œé‡‡é›†
    try:
        count = asyncio.run(crawl_weibo_data(
            keyword=args.keyword,
            limit=args.limit,
            cookies=args.cookies
        ))
        print(f"\nâœ… é‡‡é›†å®Œæˆ! å…±é‡‡é›† {count} æ¡å¾®åšæ•°æ®")
        print(f"ğŸ“Š æ•°æ®ä¿å­˜åœ¨: ./data/sentiment.db")
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­é‡‡é›†")
    except Exception as e:
        logger.exception("é‡‡é›†å¤±è´¥")
        print(f"\nâŒ é‡‡é›†å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
