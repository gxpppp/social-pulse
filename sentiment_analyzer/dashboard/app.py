"""
Streamlit å¯è§†åŒ–å¹³å°

æä¾›ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æçš„å¤šé¡µé¢å¯è§†åŒ–ç•Œé¢ï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®æ¦‚è§ˆä»ªè¡¨ç›˜
- è¶‹åŠ¿åˆ†æ
- ç½‘ç»œæ‹“æ‰‘å¯è§†åŒ–
- å¼‚å¸¸æ£€æµ‹
- è·¨äº‹ä»¶åˆ†æ
- æ•°æ®å¯¼å‡º
"""

import asyncio
import io
import json
import sqlite3
from contextlib import contextmanager
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Optional

import networkx as nx
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st
from loguru import logger
from plotly.subplots import make_subplots
from pyecharts import options as opts
from pyecharts.charts import Graph, Line, Pie, Bar

from ..config.settings import get_settings


@dataclass
class FilterConfig:
    platform: str
    time_range: str
    start_date: Optional[datetime]
    end_date: Optional[datetime]


class DashboardData:
    def __init__(self, db_path: str = "./data/sentiment.db"):
        self.db_path = db_path
        self._conn: Optional[sqlite3.Connection] = None

    @contextmanager
    def get_connection(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        try:
            yield conn
        finally:
            conn.close()

    def get_overview_stats(self, platform: str = "å…¨éƒ¨", days: int = 30) -> dict[str, Any]:
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            platform_filter = "" if platform == "å…¨éƒ¨" else f"AND platform = '{platform}'"
            date_filter = f"AND posted_at >= datetime('now', '-{days} days')"
            
            try:
                cursor.execute(f"""
                    SELECT COUNT(*) as total_posts,
                           COUNT(DISTINCT user_id) as active_users
                    FROM posts 
                    WHERE 1=1 {platform_filter} {date_filter}
                """)
                post_stats = dict(cursor.fetchone())
            except Exception:
                post_stats = {"total_posts": 0, "active_users": 0}
            
            try:
                cursor.execute(f"""
                    SELECT COUNT(*) as anomaly_count
                    FROM users 
                    WHERE is_suspicious = 1 {platform_filter.replace('platform', 'platform')}
                """)
                anomaly_count = cursor.fetchone()["anomaly_count"]
            except Exception:
                anomaly_count = 0
            
            try:
                cursor.execute(f"""
                    SELECT value, COUNT(*) as count
                    FROM posts, json_each(hashtags)
                    WHERE hashtags IS NOT NULL {platform_filter} {date_filter}
                    GROUP BY value
                    ORDER BY count DESC
                    LIMIT 10
                """)
                trending_topics = [row["value"] for row in cursor.fetchall()]
            except Exception:
                trending_topics = []
            
            return {
                "total_posts": post_stats.get("total_posts", 0),
                "active_users": post_stats.get("active_users", 0),
                "anomaly_accounts": anomaly_count,
                "trending_topics": trending_topics[:5]
            }

    def get_platform_distribution(self) -> pd.DataFrame:
        with self.get_connection() as conn:
            try:
                df = pd.read_sql_query("""
                    SELECT platform, COUNT(*) as count
                    FROM posts
                    GROUP BY platform
                    ORDER BY count DESC
                """, conn)
                return df
            except Exception:
                return pd.DataFrame({"platform": ["Twitter", "å¾®åš", "Reddit", "Telegram"], 
                                    "count": [5000, 4000, 2500, 845]})

    def get_time_series_data(self, platform: str = "å…¨éƒ¨", days: int = 30) -> pd.DataFrame:
        with self.get_connection() as conn:
            platform_filter = "" if platform == "å…¨éƒ¨" else f"AND platform = '{platform}'"
            try:
                df = pd.read_sql_query(f"""
                    SELECT date(posted_at) as date, 
                           COUNT(*) as post_count,
                           COUNT(DISTINCT user_id) as active_users
                    FROM posts
                    WHERE posted_at >= datetime('now', '-{days} days')
                    {platform_filter}
                    GROUP BY date(posted_at)
                    ORDER BY date
                """, conn)
                return df
            except Exception:
                dates = pd.date_range(end=datetime.now(), periods=days, freq="D")
                return pd.DataFrame({
                    "date": dates,
                    "post_count": np.random.randint(100, 500, days),
                    "active_users": np.random.randint(50, 200, days)
                })

    def get_sentiment_trend(self, platform: str = "å…¨éƒ¨", days: int = 30) -> pd.DataFrame:
        with self.get_connection() as conn:
            platform_filter = "" if platform == "å…¨éƒ¨" else f"AND platform = '{platform}'"
            try:
                df = pd.read_sql_query(f"""
                    SELECT date(posted_at) as date,
                           AVG(sentiment_score) as avg_sentiment,
                           COUNT(*) as count
                    FROM posts
                    WHERE posted_at >= datetime('now', '-{days} days')
                    AND sentiment_score IS NOT NULL
                    {platform_filter}
                    GROUP BY date(posted_at)
                    ORDER BY date
                """, conn)
                return df
            except Exception:
                dates = pd.date_range(end=datetime.now(), periods=days, freq="D")
                return pd.DataFrame({
                    "date": dates,
                    "avg_sentiment": np.random.uniform(0.3, 0.7, days),
                    "count": np.random.randint(100, 500, days)
                })

    def get_engagement_trend(self, platform: str = "å…¨éƒ¨", days: int = 30) -> pd.DataFrame:
        with self.get_connection() as conn:
            platform_filter = "" if platform == "å…¨éƒ¨" else f"AND platform = '{platform}'"
            try:
                df = pd.read_sql_query(f"""
                    SELECT date(posted_at) as date,
                           SUM(likes_count) as total_likes,
                           SUM(shares_count) as total_shares,
                           SUM(comments_count) as total_comments
                    FROM posts
                    WHERE posted_at >= datetime('now', '-{days} days')
                    {platform_filter}
                    GROUP BY date(posted_at)
                    ORDER BY date
                """, conn)
                return df
            except Exception:
                dates = pd.date_range(end=datetime.now(), periods=days, freq="D")
                return pd.DataFrame({
                    "date": dates,
                    "total_likes": np.random.randint(1000, 5000, days),
                    "total_shares": np.random.randint(500, 2000, days),
                    "total_comments": np.random.randint(200, 1000, days)
                })

    def get_topic_trend(self, topic: str, days: int = 30) -> pd.DataFrame:
        with self.get_connection() as conn:
            try:
                df = pd.read_sql_query(f"""
                    SELECT date(posted_at) as date, COUNT(*) as count
                    FROM posts, json_each(hashtags)
                    WHERE value = '{topic}'
                    AND posted_at >= datetime('now', '-{days} days')
                    GROUP BY date(posted_at)
                    ORDER BY date
                """, conn)
                return df
            except Exception:
                dates = pd.date_range(end=datetime.now(), periods=days, freq="D")
                return pd.DataFrame({
                    "date": dates,
                    "count": np.random.randint(10, 100, days)
                })

    def get_network_data(self, limit: int = 200) -> tuple[list[dict], list[dict]]:
        with self.get_connection() as conn:
            try:
                nodes_df = pd.read_sql_query(f"""
                    SELECT u.user_id as id, u.username as name, u.platform,
                           u.followers_count, u.is_suspicious,
                           COALESCE(uf.anomaly_score, 0) as anomaly_score,
                           COALESCE(uf.degree_centrality, 0) as centrality
                    FROM users u
                    LEFT JOIN user_features uf ON u.user_id = uf.user_id
                    ORDER BY u.followers_count DESC
                    LIMIT {limit}
                """, conn)
                
                nodes = nodes_df.to_dict("records")
                user_ids = nodes_df["id"].tolist()
                
                edges_df = pd.read_sql_query(f"""
                    SELECT 
                        i.user_id as source,
                        p.user_id as target,
                        i.interaction_type as relation,
                        COUNT(*) as weight
                    FROM interactions i
                    JOIN posts p ON i.post_id = p.post_id
                    WHERE i.user_id IN ({','.join(['?']*len(user_ids))})
                    AND p.user_id IN ({','.join(['?']*len(user_ids))})
                    AND i.user_id != p.user_id
                    GROUP BY i.user_id, p.user_id, i.interaction_type
                """, conn, params=user_ids + user_ids)
                
                edges = edges_df.to_dict("records")
                
                return nodes, edges
            except Exception:
                return self._generate_mock_network_data()

    def _generate_mock_network_data(self) -> tuple[list[dict], list[dict]]:
        nodes = []
        for i in range(50):
            nodes.append({
                "id": f"user_{i}",
                "name": f"User_{i}",
                "platform": np.random.choice(["twitter", "weibo", "reddit"]),
                "followers_count": np.random.randint(100, 10000),
                "is_suspicious": np.random.random() > 0.9,
                "anomaly_score": np.random.random(),
                "centrality": np.random.random()
            })
        
        edges = []
        for i in range(100):
            source = np.random.randint(0, 50)
            target = np.random.randint(0, 50)
            if source != target:
                edges.append({
                    "source": f"user_{source}",
                    "target": f"user_{target}",
                    "relation": np.random.choice(["retweet", "mention", "reply"]),
                    "weight": np.random.randint(1, 10)
                })
        
        return nodes, edges

    def get_anomaly_accounts(self, threshold: float = 0.5, limit: int = 100) -> pd.DataFrame:
        with self.get_connection() as conn:
            try:
                df = pd.read_sql_query(f"""
                    SELECT u.user_id, u.username, u.platform, u.followers_count,
                           u.posts_count, u.is_suspicious,
                           uf.anomaly_score, uf.predicted_label, uf.confidence_score,
                           uf.daily_post_avg, uf.content_similarity_avg,
                           uf.night_activity_ratio
                    FROM users u
                    JOIN user_features uf ON u.user_id = uf.user_id
                    WHERE uf.anomaly_score >= ?
                    ORDER BY uf.anomaly_score DESC
                    LIMIT ?
                """, conn, params=[threshold, limit])
                return df
            except Exception:
                return self._generate_mock_anomaly_data()

    def _generate_mock_anomaly_data(self) -> pd.DataFrame:
        data = []
        for i in range(20):
            data.append({
                "user_id": f"anomaly_{i}",
                "username": f"suspicious_user_{i}",
                "platform": np.random.choice(["twitter", "weibo", "reddit"]),
                "followers_count": np.random.randint(10, 1000),
                "posts_count": np.random.randint(100, 5000),
                "is_suspicious": True,
                "anomaly_score": np.random.uniform(0.7, 0.99),
                "predicted_label": np.random.choice(["bot", "troll", "spammer"]),
                "confidence_score": np.random.uniform(0.6, 0.95),
                "daily_post_avg": np.random.uniform(20, 100),
                "content_similarity_avg": np.random.uniform(0.7, 0.99),
                "night_activity_ratio": np.random.uniform(0.5, 0.9)
            })
        return pd.DataFrame(data)

    def get_feature_importance(self) -> pd.DataFrame:
        features = [
            ("daily_post_avg", 0.25),
            ("content_similarity_avg", 0.20),
            ("night_activity_ratio", 0.15),
            ("follower_ratio", 0.12),
            ("hour_entropy", 0.10),
            ("mention_ratio", 0.08),
            ("url_ratio", 0.06),
            ("sentiment_variance", 0.04)
        ]
        return pd.DataFrame(features, columns=["feature", "importance"])

    def get_events(self) -> list[dict]:
        with self.get_connection() as conn:
            try:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT DISTINCT 
                        date(posted_at) as event_date,
                        platform,
                        COUNT(*) as post_count
                    FROM posts
                    GROUP BY date(posted_at), platform
                    HAVING post_count > 100
                    ORDER BY event_date DESC
                    LIMIT 20
                """)
                return [dict(row) for row in cursor.fetchall()]
            except Exception:
                return [
                    {"event_date": "2024-01-15", "platform": "twitter", "post_count": 500},
                    {"event_date": "2024-01-14", "platform": "weibo", "post_count": 350},
                    {"event_date": "2024-01-13", "platform": "reddit", "post_count": 200}
                ]

    def get_cross_event_accounts(self, event1: str, event2: str) -> pd.DataFrame:
        with self.get_connection() as conn:
            try:
                df = pd.read_sql_query("""
                    SELECT DISTINCT u.user_id, u.username, u.platform,
                           COUNT(DISTINCT date(p.posted_at)) as event_count
                    FROM users u
                    JOIN posts p ON u.user_id = p.user_id
                    WHERE date(p.posted_at) IN (?, ?)
                    GROUP BY u.user_id
                    HAVING event_count = 2
                """, conn, params=[event1, event2])
                return df
            except Exception:
                return pd.DataFrame({
                    "user_id": [f"cross_{i}" for i in range(10)],
                    "username": [f"cross_user_{i}" for i in range(10)],
                    "platform": np.random.choice(["twitter", "weibo"], 10),
                    "event_count": [2] * 10
                })

    def get_all_posts(self, platform: str = "å…¨éƒ¨", days: int = 30, limit: int = 1000) -> pd.DataFrame:
        with self.get_connection() as conn:
            platform_filter = "" if platform == "å…¨éƒ¨" else f"AND platform = '{platform}'"
            try:
                df = pd.read_sql_query(f"""
                    SELECT p.post_id, p.user_id, p.platform, p.content,
                           p.posted_at, p.likes_count, p.shares_count,
                           p.comments_count, p.hashtags, p.sentiment_score,
                           u.username
                    FROM posts p
                    JOIN users u ON p.user_id = u.user_id
                    WHERE p.posted_at >= datetime('now', '-{days} days')
                    {platform_filter}
                    ORDER BY p.posted_at DESC
                    LIMIT ?
                """, conn, params=[limit])
                return df
            except Exception:
                return pd.DataFrame()

    def get_all_users(self, platform: str = "å…¨éƒ¨", limit: int = 1000) -> pd.DataFrame:
        with self.get_connection() as conn:
            platform_filter = "" if platform == "å…¨éƒ¨" else f"AND platform = '{platform}'"
            try:
                df = pd.read_sql_query(f"""
                    SELECT user_id, username, platform, followers_count,
                           friends_count, posts_count, verified, is_suspicious
                    FROM users
                    WHERE 1=1 {platform_filter}
                    ORDER BY followers_count DESC
                    LIMIT ?
                """, conn, params=[limit])
                return df
            except Exception:
                return pd.DataFrame()


def get_time_range_days(time_range: str) -> int:
    mapping = {
        "æœ€è¿‘24å°æ—¶": 1,
        "æœ€è¿‘7å¤©": 7,
        "æœ€è¿‘30å¤©": 30,
        "æœ€è¿‘90å¤©": 90,
        "å…¨éƒ¨": 365
    }
    return mapping.get(time_range, 30)


@st.cache_resource
def get_data_provider() -> DashboardData:
    settings = get_settings()
    db_path = settings.database_url.replace("sqlite:///", "")
    return DashboardData(db_path)


@st.cache_data(ttl=300)
def cached_overview_stats(platform: str, days: int) -> dict[str, Any]:
    return get_data_provider().get_overview_stats(platform, days)


@st.cache_data(ttl=300)
def cached_platform_distribution() -> pd.DataFrame:
    return get_data_provider().get_platform_distribution()


@st.cache_data(ttl=300)
def cached_time_series(platform: str, days: int) -> pd.DataFrame:
    return get_data_provider().get_time_series_data(platform, days)


@st.cache_data(ttl=300)
def cached_sentiment_trend(platform: str, days: int) -> pd.DataFrame:
    return get_data_provider().get_sentiment_trend(platform, days)


@st.cache_data(ttl=300)
def cached_engagement_trend(platform: str, days: int) -> pd.DataFrame:
    return get_data_provider().get_engagement_trend(platform, days)


@st.cache_data(ttl=300)
def cached_network_data(limit: int) -> tuple[list[dict], list[dict]]:
    return get_data_provider().get_network_data(limit)


@st.cache_data(ttl=300)
def cached_anomaly_accounts(threshold: float, limit: int) -> pd.DataFrame:
    return get_data_provider().get_anomaly_accounts(threshold, limit)


def render_sidebar() -> FilterConfig:
    with st.sidebar:
        st.header("âš™ï¸ æ§åˆ¶é¢æ¿")
        
        platform = st.selectbox(
            "é€‰æ‹©å¹³å°",
            ["å…¨éƒ¨", "Twitter", "å¾®åš", "Reddit", "Telegram"],
            key="platform_select"
        )
        
        time_range = st.selectbox(
            "æ—¶é—´èŒƒå›´",
            ["æœ€è¿‘24å°æ—¶", "æœ€è¿‘7å¤©", "æœ€è¿‘30å¤©", "æœ€è¿‘90å¤©", "å…¨éƒ¨"],
            key="time_range_select"
        )
        
        st.divider()
        
        analysis_type = st.radio(
            "åˆ†æç±»å‹",
            ["æ¦‚è§ˆ", "è¶‹åŠ¿åˆ†æ", "ç½‘ç»œæ‹“æ‰‘", "å¼‚å¸¸æ£€æµ‹", "è·¨äº‹ä»¶åˆ†æ"],
            key="analysis_type_radio"
        )
        
        st.divider()
        
        if st.button("ğŸ”„ åˆ·æ–°æ•°æ®", use_container_width=True):
            st.cache_data.clear()
            st.rerun()
        
        st.divider()
        
        with st.expander("ğŸ“Š æ•°æ®å¯¼å‡º"):
            export_format = st.selectbox("å¯¼å‡ºæ ¼å¼", ["CSV", "JSON", "PDFæŠ¥å‘Š"])
            if st.button("ğŸ“¥ å¯¼å‡ºæ•°æ®", use_container_width=True):
                handle_export(export_format)
        
        days = get_time_range_days(time_range)
        
        return FilterConfig(
            platform=platform,
            time_range=time_range,
            start_date=datetime.now() - timedelta(days=days),
            end_date=datetime.now()
        )


def handle_export(export_format: str) -> None:
    try:
        data_provider = get_data_provider()
        platform = st.session_state.get("platform_select", "å…¨éƒ¨")
        time_range = st.session_state.get("time_range_select", "æœ€è¿‘30å¤©")
        days = get_time_range_days(time_range)
        
        if export_format == "CSV":
            posts_df = data_provider.get_all_posts(platform, days)
            users_df = data_provider.get_all_users(platform)
            
            csv_buffer = io.StringIO()
            if not posts_df.empty:
                csv_buffer.write("# å¸–å­æ•°æ®\n")
                csv_buffer.write(posts_df.to_csv(index=False))
                csv_buffer.write("\n")
            if not users_df.empty:
                csv_buffer.write("# ç”¨æˆ·æ•°æ®\n")
                csv_buffer.write(users_df.to_csv(index=False))
            
            st.download_button(
                label="ä¸‹è½½ CSV æ–‡ä»¶",
                data=csv_buffer.getvalue(),
                file_name=f"sentiment_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
            
        elif export_format == "JSON":
            posts_df = data_provider.get_all_posts(platform, days)
            users_df = data_provider.get_all_users(platform)
            
            export_data = {
                "export_time": datetime.now().isoformat(),
                "platform": platform,
                "time_range": time_range,
                "posts": posts_df.to_dict("records") if not posts_df.empty else [],
                "users": users_df.to_dict("records") if not users_df.empty else []
            }
            
            st.download_button(
                label="ä¸‹è½½ JSON æ–‡ä»¶",
                data=json.dumps(export_data, ensure_ascii=False, indent=2, default=str),
                file_name=f"sentiment_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json"
            )
            
        elif export_format == "PDFæŠ¥å‘Š":
            st.info("PDFæŠ¥å‘Šå¯¼å‡ºåŠŸèƒ½éœ€è¦å®‰è£… reportlab åº“ã€‚å½“å‰æ˜¾ç¤ºæŠ¥å‘Šé¢„è§ˆã€‚")
            
            stats = data_provider.get_overview_stats(platform, days)
            
            report_content = f"""
# ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†ææŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- å¹³å°: {platform}
- æ—¶é—´èŒƒå›´: {time_range}

## æ•°æ®æ¦‚è§ˆ
- æ€»å¸–å­æ•°: {stats['total_posts']:,}
- æ´»è·ƒç”¨æˆ·: {stats['active_users']:,}
- å¼‚å¸¸è´¦å·: {stats['anomaly_accounts']:,}
- çƒ­é—¨è¯é¢˜: {', '.join(stats['trending_topics']) if stats['trending_topics'] else 'æ— '}

## åˆ†ææ‘˜è¦
æœ¬æŠ¥å‘ŠåŸºäºæŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„ç¤¾äº¤åª’ä½“æ•°æ®è¿›è¡Œåˆ†æï¼ŒåŒ…å«æƒ…æ„Ÿè¶‹åŠ¿ã€ç”¨æˆ·è¡Œä¸ºå’Œç½‘ç»œæ‹“æ‰‘ç­‰å¤šç»´åº¦åˆ†æç»“æœã€‚
"""
            st.download_button(
                label="ä¸‹è½½æŠ¥å‘Š (Markdown)",
                data=report_content,
                file_name=f"sentiment_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                mime="text/markdown"
            )
            
    except Exception as e:
        st.error(f"å¯¼å‡ºå¤±è´¥: {str(e)}")


def render_overview_page(filter_config: FilterConfig) -> None:
    st.header("ğŸ“Š æ•°æ®æ¦‚è§ˆ")
    
    days = get_time_range_days(filter_config.time_range)
    
    with st.spinner("åŠ è½½æ•°æ®ä¸­..."):
        stats = cached_overview_stats(filter_config.platform, days)
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            label="ğŸ“ æ€»å¸–å­æ•°",
            value=f"{stats['total_posts']:,}",
            delta=None
        )
    
    with col2:
        st.metric(
            label="ğŸ‘¥ æ´»è·ƒç”¨æˆ·",
            value=f"{stats['active_users']:,}",
            delta=None
        )
    
    with col3:
        st.metric(
            label="âš ï¸ å¼‚å¸¸è´¦å·",
            value=f"{stats['anomaly_accounts']:,}",
            delta=None
        )
    
    with col4:
        trending = stats.get('trending_topics', [])
        st.metric(
            label="ğŸ”¥ çƒ­é—¨è¯é¢˜",
            value=len(trending),
            delta=None
        )
    
    st.divider()
    
    col_left, col_right = st.columns(2)
    
    with col_left:
        st.subheader("å¹³å°åˆ†å¸ƒ")
        platform_df = cached_platform_distribution()
        
        if not platform_df.empty:
            fig_pie = px.pie(
                platform_df,
                values="count",
                names="platform",
                color_discrete_sequence=px.colors.qualitative.Set2,
                hole=0.4
            )
            fig_pie.update_traces(textposition="inside", textinfo="percent+label")
            fig_pie.update_layout(
                margin=dict(t=20, b=20, l=20, r=20),
                height=350
            )
            st.plotly_chart(fig_pie, use_container_width=True)
        else:
            st.info("æš‚æ— å¹³å°åˆ†å¸ƒæ•°æ®")
    
    with col_right:
        st.subheader("å¸–å­æ•°é‡è¶‹åŠ¿")
        time_series_df = cached_time_series(filter_config.platform, days)
        
        if not time_series_df.empty:
            fig_line = px.line(
                time_series_df,
                x="date",
                y="post_count",
                markers=True,
                color_discrete_sequence=["#1f77b4"]
            )
            fig_line.update_layout(
                xaxis_title="æ—¥æœŸ",
                yaxis_title="å¸–å­æ•°é‡",
                margin=dict(t=20, b=20, l=20, r=20),
                height=350
            )
            st.plotly_chart(fig_line, use_container_width=True)
        else:
            st.info("æš‚æ— è¶‹åŠ¿æ•°æ®")
    
    st.subheader("ç”¨æˆ·æ´»è·ƒåº¦è¶‹åŠ¿")
    if not time_series_df.empty:
        fig_area = go.Figure()
        fig_area.add_trace(go.Scatter(
            x=time_series_df["date"],
            y=time_series_df["active_users"],
            fill="tozeroy",
            mode="lines",
            name="æ´»è·ƒç”¨æˆ·",
            line=dict(color="#2ecc71")
        ))
        fig_area.update_layout(
            xaxis_title="æ—¥æœŸ",
            yaxis_title="ç”¨æˆ·æ•°",
            hovermode="x unified",
            margin=dict(t=20, b=20, l=20, r=20),
            height=300
        )
        st.plotly_chart(fig_area, use_container_width=True)
    
    if trending:
        st.subheader("çƒ­é—¨è¯é¢˜")
        topics_df = pd.DataFrame({
            "è¯é¢˜": trending,
            "çƒ­åº¦": [100 - i * 10 for i in range(len(trending))]
        })
        fig_topics = px.bar(
            topics_df,
            x="çƒ­åº¦",
            y="è¯é¢˜",
            orientation="h",
            color="çƒ­åº¦",
            color_continuous_scale="Viridis"
        )
        fig_topics.update_layout(
            margin=dict(t=20, b=20, l=20, r=20),
            height=300,
            showlegend=False
        )
        st.plotly_chart(fig_topics, use_container_width=True)


def render_trend_analysis_page(filter_config: FilterConfig) -> None:
    st.header("ğŸ“ˆ è¶‹åŠ¿åˆ†æ")
    
    days = get_time_range_days(filter_config.time_range)
    
    tab1, tab2, tab3 = st.tabs(["æ—¶é—´åºåˆ—", "è¯é¢˜çƒ­åº¦", "æƒ…æ„Ÿåˆ†æ"])
    
    with tab1:
        st.subheader("å¸–å­å‘å¸ƒæ—¶é—´åºåˆ—")
        
        time_series_df = cached_time_series(filter_config.platform, days)
        
        if not time_series_df.empty:
            fig = make_subplots(
                rows=2, cols=1,
                subplot_titles=("å¸–å­æ•°é‡", "æ´»è·ƒç”¨æˆ·"),
                vertical_spacing=0.15
            )
            
            fig.add_trace(
                go.Scatter(
                    x=time_series_df["date"],
                    y=time_series_df["post_count"],
                    mode="lines+markers",
                    name="å¸–å­æ•°é‡",
                    line=dict(color="#3498db", width=2)
                ),
                row=1, col=1
            )
            
            fig.add_trace(
                go.Scatter(
                    x=time_series_df["date"],
                    y=time_series_df["active_users"],
                    mode="lines+markers",
                    name="æ´»è·ƒç”¨æˆ·",
                    line=dict(color="#2ecc71", width=2)
                ),
                row=2, col=1
            )
            
            fig.update_layout(
                height=500,
                showlegend=True,
                margin=dict(t=30, b=20, l=20, r=20)
            )
            fig.update_xaxes(title_text="æ—¥æœŸ", row=2, col=1)
            fig.update_yaxes(title_text="æ•°é‡", row=1, col=1)
            fig.update_yaxes(title_text="ç”¨æˆ·æ•°", row=2, col=1)
            
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("æš‚æ— æ—¶é—´åºåˆ—æ•°æ®")
    
    with tab2:
        st.subheader("è¯é¢˜çƒ­åº¦è¶‹åŠ¿")
        
        stats = cached_overview_stats(filter_config.platform, days)
        trending = stats.get('trending_topics', [])
        
        if trending:
            selected_topic = st.selectbox("é€‰æ‹©è¯é¢˜", trending, key="topic_select")
            
            topic_df = get_data_provider().get_topic_trend(selected_topic, days)
            
            if not topic_df.empty:
                fig_topic = px.line(
                    topic_df,
                    x="date",
                    y="count",
                    markers=True,
                    title=f"'{selected_topic}' è¯é¢˜è¶‹åŠ¿"
                )
                fig_topic.update_layout(
                    xaxis_title="æ—¥æœŸ",
                    yaxis_title="æåŠæ¬¡æ•°",
                    margin=dict(t=40, b=20, l=20, r=20)
                )
                st.plotly_chart(fig_topic, use_container_width=True)
            else:
                st.info(f"æš‚æ—  '{selected_topic}' çš„è¶‹åŠ¿æ•°æ®")
        else:
            st.info("æš‚æ— çƒ­é—¨è¯é¢˜æ•°æ®")
    
    with tab3:
        st.subheader("æƒ…æ„Ÿåˆ†æè¶‹åŠ¿")
        
        sentiment_df = cached_sentiment_trend(filter_config.platform, days)
        
        if not sentiment_df.empty:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                fig_sentiment = go.Figure()
                fig_sentiment.add_trace(go.Scatter(
                    x=sentiment_df["date"],
                    y=sentiment_df["avg_sentiment"],
                    mode="lines+markers",
                    name="å¹³å‡æƒ…æ„Ÿåˆ†æ•°",
                    line=dict(color="#9b59b6", width=2),
                    fill="tozeroy",
                    fillcolor="rgba(155, 89, 182, 0.2)"
                ))
                
                fig_sentiment.add_hline(
                    y=0.5,
                    line_dash="dash",
                    line_color="gray",
                    annotation_text="ä¸­æ€§çº¿"
                )
                
                fig_sentiment.update_layout(
                    xaxis_title="æ—¥æœŸ",
                    yaxis_title="æƒ…æ„Ÿåˆ†æ•°",
                    yaxis=dict(range=[0, 1]),
                    margin=dict(t=20, b=20, l=20, r=20),
                    height=400
                )
                st.plotly_chart(fig_sentiment, use_container_width=True)
            
            with col2:
                avg_sentiment = sentiment_df["avg_sentiment"].mean()
                sentiment_label = "ç§¯æ" if avg_sentiment > 0.6 else "æ¶ˆæ" if avg_sentiment < 0.4 else "ä¸­æ€§"
                sentiment_color = "#2ecc71" if avg_sentiment > 0.6 else "#e74c3c" if avg_sentiment < 0.4 else "#f39c12"
                
                st.metric("å¹³å‡æƒ…æ„Ÿåˆ†æ•°", f"{avg_sentiment:.3f}")
                st.metric("æƒ…æ„Ÿå€¾å‘", sentiment_label)
                
                fig_gauge = go.Figure(go.Indicator(
                    mode="gauge+number",
                    value=avg_sentiment,
                    domain={"x": [0, 1], "y": [0, 1]},
                    gauge={
                        "axis": {"range": [0, 1]},
                        "bar": {"color": sentiment_color},
                        "steps": [
                            {"range": [0, 0.4], "color": "#fadbd8"},
                            {"range": [0.4, 0.6], "color": "#fdebd0"},
                            {"range": [0.6, 1], "color": "#d5f5e3"}
                        ]
                    }
                ))
                fig_gauge.update_layout(height=250, margin=dict(t=20, b=20, l=20, r=20))
                st.plotly_chart(fig_gauge, use_container_width=True)
        else:
            st.info("æš‚æ— æƒ…æ„Ÿåˆ†ææ•°æ®")
        
        st.divider()
        st.subheader("äº’åŠ¨æ•°æ®è¶‹åŠ¿")
        
        engagement_df = cached_engagement_trend(filter_config.platform, days)
        
        if not engagement_df.empty:
            fig_engagement = go.Figure()
            
            fig_engagement.add_trace(go.Scatter(
                x=engagement_df["date"],
                y=engagement_df["total_likes"],
                mode="lines",
                name="ç‚¹èµ",
                stackgroup="one",
                line=dict(color="#3498db")
            ))
            fig_engagement.add_trace(go.Scatter(
                x=engagement_df["date"],
                y=engagement_df["total_shares"],
                mode="lines",
                name="è½¬å‘",
                stackgroup="one",
                line=dict(color="#2ecc71")
            ))
            fig_engagement.add_trace(go.Scatter(
                x=engagement_df["date"],
                y=engagement_df["total_comments"],
                mode="lines",
                name="è¯„è®º",
                stackgroup="one",
                line=dict(color="#e74c3c")
            ))
            
            fig_engagement.update_layout(
                xaxis_title="æ—¥æœŸ",
                yaxis_title="äº’åŠ¨æ•°é‡",
                hovermode="x unified",
                margin=dict(t=20, b=20, l=20, r=20),
                height=350
            )
            st.plotly_chart(fig_engagement, use_container_width=True)
        else:
            st.info("æš‚æ— äº’åŠ¨æ•°æ®")


def render_network_page(filter_config: FilterConfig) -> None:
    st.header("ğŸ•¸ï¸ ç½‘ç»œæ‹“æ‰‘å¯è§†åŒ–")
    
    with st.sidebar:
        st.divider()
        st.subheader("ç½‘ç»œå›¾è®¾ç½®")
        node_limit = st.slider("èŠ‚ç‚¹æ•°é‡é™åˆ¶", 50, 500, 200, key="node_limit")
        show_labels = st.checkbox("æ˜¾ç¤ºèŠ‚ç‚¹æ ‡ç­¾", value=True, key="show_labels")
        highlight_suspicious = st.checkbox("é«˜äº®å¼‚å¸¸è´¦å·", value=True, key="highlight_suspicious")
    
    with st.spinner("åŠ è½½ç½‘ç»œæ•°æ®..."):
        nodes, edges = cached_network_data(node_limit)
    
    if not nodes:
        st.warning("æš‚æ— ç½‘ç»œæ•°æ®")
        return
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        st.subheader("äº¤äº’ç½‘ç»œå›¾")
        
        node_categories = {
            "normal": 0,
            "suspicious": 1,
            "influencer": 2
        }
        
        pyecharts_nodes = []
        for node in nodes:
            category = 0
            if node.get("is_suspicious") and highlight_suspicious:
                category = 1
            elif node.get("followers_count", 0) > 5000:
                category = 2
            
            node_size = max(10, min(50, node.get("centrality", 0.5) * 40 + 10))
            
            pyecharts_nodes.append(
                opts.GraphNodeOpts(
                    name=node.get("name", node.get("id")),
                    symbol_size=node_size,
                    category=category,
                    value=node.get("centrality", 0),
                    label_opts=opts.LabelOpts(
                        is_show=show_labels,
                        position="right",
                        font_size=10
                    )
                )
            )
        
        pyecharts_edges = []
        for edge in edges:
            source_node = next((n for n in nodes if n["id"] == edge["source"]), None)
            target_node = next((n for n in nodes if n["id"] == edge["target"]), None)
            
            if source_node and target_node:
                pyecharts_edges.append(
                    opts.GraphLinkOpts(
                        source=source_node.get("name", edge["source"]),
                        target=target_node.get("name", edge["target"]),
                        value=edge.get("weight", 1)
                    )
                )
        
        categories = [
            opts.GraphCategoryOpts(name="æ™®é€šç”¨æˆ·", itemstyle_opts=opts.ItemStyleOpts(color="#5dade2")),
            opts.GraphCategoryOpts(name="å¼‚å¸¸è´¦å·", itemstyle_opts=opts.ItemStyleOpts(color="#e74c3c")),
            opts.GraphCategoryOpts(name="å½±å“åŠ›ç”¨æˆ·", itemstyle_opts=opts.ItemStyleOpts(color="#f39c12"))
        ]
        
        graph = (
            Graph(init_opts=opts.InitOpts(width="100%", height="600px"))
            .add(
                "",
                nodes=pyecharts_nodes,
                links=pyecharts_edges,
                categories=categories,
                layout="force",
                is_roam=True,
                is_focusnode=True,
                is_draggable=True,
                repulsion=1000,
                gravity=0.1,
                edge_length=[50, 200],
                linestyle_opts=opts.LineStyleOpts(
                    width=0.5,
                    curve=0.2,
                    opacity=0.6
                ),
                edge_symbol=["circle", "arrow"],
                edge_symbol_size=[4, 8]
            )
            .set_global_opts(
                title_opts=opts.TitleOpts(title="ç¤¾äº¤ç½‘ç»œå…³ç³»å›¾"),
                legend_opts=opts.LegendOpts(
                    orient="vertical",
                    pos_left="left",
                    pos_top="middle"
                ),
                toolbox_opts=opts.ToolboxOpts(
                    is_show=True,
                    feature={
                        "saveAsImage": {"title": "ä¿å­˜å›¾ç‰‡"},
                        "restore": {"title": "è¿˜åŸ"},
                        "dataZoom": {"title": "ç¼©æ”¾"}
                    }
                )
            )
        )
        
        st.components.v1.html(graph.render_embed(), height=650)
    
    with col2:
        st.subheader("ç½‘ç»œç»Ÿè®¡")
        
        G = nx.DiGraph()
        for node in nodes:
            G.add_node(node["id"], **node)
        for edge in edges:
            G.add_edge(edge["source"], edge["target"], weight=edge.get("weight", 1))
        
        st.metric("èŠ‚ç‚¹æ•°", G.number_of_nodes())
        st.metric("è¾¹æ•°", G.number_of_edges())
        
        if G.number_of_nodes() > 0:
            density = nx.density(G)
            st.metric("ç½‘ç»œå¯†åº¦", f"{density:.4f}")
            
            if nx.is_weakly_connected(G):
                largest_cc = max(nx.weakly_connected_components(G), key=len)
                st.metric("æœ€å¤§è¿é€šåˆ†é‡", len(largest_cc))
        
        st.divider()
        st.subheader("èŠ‚ç‚¹æœç´¢")
        search_term = st.text_input("æœç´¢ç”¨æˆ·", key="node_search")
        
        if search_term:
            matching_nodes = [
                n for n in nodes 
                if search_term.lower() in n.get("name", "").lower() 
                or search_term.lower() in n.get("id", "").lower()
            ]
            
            if matching_nodes:
                st.write(f"æ‰¾åˆ° {len(matching_nodes)} ä¸ªåŒ¹é…èŠ‚ç‚¹:")
                for node in matching_nodes[:10]:
                    status = "âš ï¸ å¼‚å¸¸" if node.get("is_suspicious") else "âœ“ æ­£å¸¸"
                    st.write(f"- {node.get('name')} ({status})")
            else:
                st.info("æœªæ‰¾åˆ°åŒ¹é…èŠ‚ç‚¹")


def render_anomaly_page(filter_config: FilterConfig) -> None:
    st.header("ğŸ” å¼‚å¸¸æ£€æµ‹")
    
    with st.sidebar:
        st.divider()
        st.subheader("å¼‚å¸¸æ£€æµ‹è®¾ç½®")
        threshold = st.slider("å¼‚å¸¸åˆ†æ•°é˜ˆå€¼", 0.0, 1.0, 0.5, 0.05, key="anomaly_threshold")
        result_limit = st.slider("ç»“æœæ•°é‡é™åˆ¶", 10, 200, 50, key="result_limit")
    
    anomaly_df = cached_anomaly_accounts(threshold, result_limit)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("æ£€æµ‹åˆ°çš„å¼‚å¸¸è´¦å·", len(anomaly_df))
    
    with col2:
        if not anomaly_df.empty:
            avg_score = anomaly_df["anomaly_score"].mean()
            st.metric("å¹³å‡å¼‚å¸¸åˆ†æ•°", f"{avg_score:.3f}")
        else:
            st.metric("å¹³å‡å¼‚å¸¸åˆ†æ•°", "N/A")
    
    with col3:
        if not anomaly_df.empty and "predicted_label" in anomaly_df.columns:
            label_counts = anomaly_df["predicted_label"].value_counts()
            most_common = label_counts.index[0] if len(label_counts) > 0 else "N/A"
            st.metric("æœ€å¸¸è§ç±»å‹", most_common)
        else:
            st.metric("æœ€å¸¸è§ç±»å‹", "N/A")
    
    tab1, tab2, tab3 = st.tabs(["å¼‚å¸¸è´¦å·åˆ—è¡¨", "åˆ†æ•°åˆ†å¸ƒ", "ç‰¹å¾é‡è¦æ€§"])
    
    with tab1:
        st.subheader("å¼‚å¸¸è´¦å·åˆ—è¡¨")
        
        if not anomaly_df.empty:
            display_cols = ["username", "platform", "anomaly_score", "predicted_label", 
                           "followers_count", "posts_count"]
            available_cols = [c for c in display_cols if c in anomaly_df.columns]
            
            df_display = anomaly_df[available_cols].copy()
            df_display = df_display.round(3)
            
            st.dataframe(
                df_display,
                use_container_width=True,
                hide_index=True,
                column_config={
                    "anomaly_score": st.column_config.ProgressColumn(
                        "å¼‚å¸¸åˆ†æ•°",
                        help="å¼‚å¸¸æ£€æµ‹åˆ†æ•°",
                        format="%.3f",
                        min_value=0,
                        max_value=1
                    ),
                    "predicted_label": st.column_config.TextColumn("é¢„æµ‹ç±»å‹"),
                    "platform": st.column_config.TextColumn("å¹³å°"),
                    "followers_count": st.column_config.NumberColumn("ç²‰ä¸æ•°", format="%d"),
                    "posts_count": st.column_config.NumberColumn("å¸–å­æ•°", format="%d")
                }
            )
            
            selected_user = st.selectbox(
                "é€‰æ‹©è´¦å·æŸ¥çœ‹è¯¦æƒ…",
                anomaly_df["user_id"].tolist() if "user_id" in anomaly_df.columns else []
            )
            
            if selected_user:
                user_data = anomaly_df[anomaly_df["user_id"] == selected_user].iloc[0]
                
                with st.expander("ğŸ“‹ è´¦å·è¯¦æƒ…", expanded=True):
                    col_a, col_b = st.columns(2)
                    
                    with col_a:
                        st.write("**åŸºæœ¬ä¿¡æ¯**")
                        st.write(f"ç”¨æˆ·ID: {user_data.get('user_id', 'N/A')}")
                        st.write(f"ç”¨æˆ·å: {user_data.get('username', 'N/A')}")
                        st.write(f"å¹³å°: {user_data.get('platform', 'N/A')}")
                        st.write(f"ç²‰ä¸æ•°: {user_data.get('followers_count', 0):,}")
                        st.write(f"å¸–å­æ•°: {user_data.get('posts_count', 0):,}")
                    
                    with col_b:
                        st.write("**å¼‚å¸¸æŒ‡æ ‡**")
                        st.write(f"å¼‚å¸¸åˆ†æ•°: {user_data.get('anomaly_score', 0):.3f}")
                        st.write(f"é¢„æµ‹ç±»å‹: {user_data.get('predicted_label', 'N/A')}")
                        st.write(f"ç½®ä¿¡åº¦: {user_data.get('confidence_score', 0):.2%}")
                        st.write(f"æ—¥å‡å‘å¸–: {user_data.get('daily_post_avg', 0):.1f}")
                        st.write(f"å†…å®¹ç›¸ä¼¼åº¦: {user_data.get('content_similarity_avg', 0):.2%}")
                        st.write(f"å¤œé—´æ´»è·ƒæ¯”: {user_data.get('night_activity_ratio', 0):.2%}")
        else:
            st.info("æœªæ£€æµ‹åˆ°å¼‚å¸¸è´¦å·")
    
    with tab2:
        st.subheader("å¼‚å¸¸åˆ†æ•°åˆ†å¸ƒ")
        
        if not anomaly_df.empty and "anomaly_score" in anomaly_df.columns:
            fig_hist = px.histogram(
                anomaly_df,
                x="anomaly_score",
                nbins=20,
                title="å¼‚å¸¸åˆ†æ•°åˆ†å¸ƒ",
                color_discrete_sequence=["#e74c3c"]
            )
            fig_hist.update_layout(
                xaxis_title="å¼‚å¸¸åˆ†æ•°",
                yaxis_title="è´¦å·æ•°é‡",
                bargap=0.1,
                margin=dict(t=40, b=20, l=20, r=20)
            )
            fig_hist.add_vline(
                x=threshold,
                line_dash="dash",
                line_color="blue",
                annotation_text=f"é˜ˆå€¼: {threshold}"
            )
            st.plotly_chart(fig_hist, use_container_width=True)
            
            if "predicted_label" in anomaly_df.columns:
                label_counts = anomaly_df["predicted_label"].value_counts().reset_index()
                label_counts.columns = ["ç±»å‹", "æ•°é‡"]
                
                fig_pie = px.pie(
                    label_counts,
                    values="æ•°é‡",
                    names="ç±»å‹",
                    title="å¼‚å¸¸ç±»å‹åˆ†å¸ƒ",
                    color_discrete_sequence=px.colors.qualitative.Set3
                )
                fig_pie.update_layout(margin=dict(t=40, b=20, l=20, r=20))
                st.plotly_chart(fig_pie, use_container_width=True)
        else:
            st.info("æš‚æ— åˆ†å¸ƒæ•°æ®")
    
    with tab3:
        st.subheader("ç‰¹å¾é‡è¦æ€§")
        
        feature_df = get_data_provider().get_feature_importance()
        
        fig_feature = px.bar(
            feature_df,
            x="importance",
            y="feature",
            orientation="h",
            title="å¼‚å¸¸æ£€æµ‹ç‰¹å¾é‡è¦æ€§",
            color="importance",
            color_continuous_scale="Viridis"
        )
        fig_feature.update_layout(
            xaxis_title="é‡è¦æ€§",
            yaxis_title="ç‰¹å¾",
            margin=dict(t=40, b=20, l=20, r=20),
            height=400
        )
        st.plotly_chart(fig_feature, use_container_width=True)
        
        st.info("""
        **ç‰¹å¾è¯´æ˜:**
        - **daily_post_avg**: æ—¥å‡å‘å¸–æ•°é‡
        - **content_similarity_avg**: å†…å®¹ç›¸ä¼¼åº¦å¹³å‡å€¼
        - **night_activity_ratio**: å¤œé—´æ´»åŠ¨æ¯”ä¾‹
        - **follower_ratio**: ç²‰ä¸/å…³æ³¨æ¯”ä¾‹
        - **hour_entropy**: å‘å¸–æ—¶é—´ç†µå€¼
        - **mention_ratio**: æåŠæ¯”ä¾‹
        - **url_ratio**: URLé“¾æ¥æ¯”ä¾‹
        - **sentiment_variance**: æƒ…æ„Ÿæ–¹å·®
        """)


def render_cross_event_page(filter_config: FilterConfig) -> None:
    st.header("ğŸ”— è·¨äº‹ä»¶åˆ†æ")
    
    events = get_data_provider().get_events()
    
    if not events:
        st.warning("æš‚æ— äº‹ä»¶æ•°æ®")
        return
    
    col1, col2 = st.columns(2)
    
    with col1:
        event_options = [f"{e['event_date']} ({e['platform']}) - {e['post_count']}å¸–å­" for e in events]
        event1_idx = st.selectbox(
            "é€‰æ‹©äº‹ä»¶1",
            range(len(event_options)),
            format_func=lambda i: event_options[i],
            key="event1_select"
        )
    
    with col2:
        event2_idx = st.selectbox(
            "é€‰æ‹©äº‹ä»¶2",
            range(len(event_options)),
            format_func=lambda i: event_options[i],
            index=min(1, len(event_options) - 1),
            key="event2_select"
        )
    
    if event1_idx == event2_idx:
        st.warning("è¯·é€‰æ‹©ä¸¤ä¸ªä¸åŒçš„äº‹ä»¶è¿›è¡Œæ¯”è¾ƒ")
        return
    
    event1 = events[event1_idx]
    event2 = events[event2_idx]
    
    st.divider()
    
    tab1, tab2, tab3 = st.tabs(["å¤ç”¨è´¦å·ç½‘ç»œ", "è¡Œä¸ºæ¼”åŒ–æ—¶é—´çº¿", "è´¦å·èµ„äº§åº“"])
    
    with tab1:
        st.subheader("å¤ç”¨è´¦å·ç½‘ç»œå›¾")
        
        cross_accounts = get_data_provider().get_cross_event_accounts(
            event1["event_date"], 
            event2["event_date"]
        )
        
        if not cross_accounts.empty:
            st.metric("è·¨äº‹ä»¶æ´»è·ƒè´¦å·", len(cross_accounts))
            
            G = nx.DiGraph()
            
            for _, row in cross_accounts.iterrows():
                G.add_node(
                    row["user_id"],
                    name=row.get("username", row["user_id"]),
                    platform=row.get("platform", "unknown")
                )
            
            for i, node1 in cross_accounts.iterrows():
                for j, node2 in cross_accounts.iterrows():
                    if i < j and node1["platform"] == node2["platform"]:
                        G.add_edge(node1["user_id"], node2["user_id"])
            
            pyecharts_nodes = []
            for node_id in G.nodes():
                node_data = G.nodes[node_id]
                pyecharts_nodes.append(
                    opts.GraphNodeOpts(
                        name=node_data.get("name", node_id),
                        symbol_size=20,
                        category=0 if node_data.get("platform") == "twitter" else 1
                    )
                )
            
            pyecharts_edges = []
            for edge in G.edges():
                source_name = G.nodes[edge[0]].get("name", edge[0])
                target_name = G.nodes[edge[1]].get("name", edge[1])
                pyecharts_edges.append(
                    opts.GraphLinkOpts(source=source_name, target=target_name)
                )
            
            categories = [
                opts.GraphCategoryOpts(name="Twitter", itemstyle_opts=opts.ItemStyleOpts(color="#1da1f2")),
                opts.GraphCategoryOpts(name="å…¶ä»–å¹³å°", itemstyle_opts=opts.ItemStyleOpts(color="#ff6b6b"))
            ]
            
            graph = (
                Graph(init_opts=opts.InitOpts(width="100%", height="500px"))
                .add(
                    "",
                    nodes=pyecharts_nodes,
                    links=pyecharts_edges,
                    categories=categories,
                    layout="force",
                    is_roam=True,
                    is_draggable=True,
                    repulsion=500
                )
                .set_global_opts(
                    title_opts=opts.TitleOpts(title="è·¨äº‹ä»¶å¤ç”¨è´¦å·ç½‘ç»œ"),
                    legend_opts=opts.LegendOpts(pos_left="left")
                )
            )
            
            st.components.v1.html(graph.render_embed(), height=550)
        else:
            st.info("æœªå‘ç°è·¨äº‹ä»¶æ´»è·ƒè´¦å·")
    
    with tab2:
        st.subheader("è¡Œä¸ºæ¼”åŒ–æ—¶é—´çº¿")
        
        timeline_data = pd.DataFrame({
            "æ—¶é—´ç‚¹": [
                event1["event_date"],
                event2["event_date"]
            ],
            "äº‹ä»¶": [
                f"äº‹ä»¶1: {event1['platform']}",
                f"äº‹ä»¶2: {event2['platform']}"
            ],
            "å¸–å­æ•°": [
                event1["post_count"],
                event2["post_count"]
            ]
        })
        
        fig_timeline = px.scatter(
            timeline_data,
            x="æ—¶é—´ç‚¹",
            y="å¸–å­æ•°",
            size="å¸–å­æ•°",
            color="äº‹ä»¶",
            title="äº‹ä»¶æ—¶é—´çº¿",
            size_max=50
        )
        fig_timeline.update_layout(
            margin=dict(t=40, b=20, l=20, r=20),
            height=400
        )
        st.plotly_chart(fig_timeline, use_container_width=True)
        
        col_a, col_b = st.columns(2)
        with col_a:
            st.write(f"**äº‹ä»¶1è¯¦æƒ…**")
            st.write(f"æ—¥æœŸ: {event1['event_date']}")
            st.write(f"å¹³å°: {event1['platform']}")
            st.write(f"å¸–å­æ•°: {event1['post_count']}")
        
        with col_b:
            st.write(f"**äº‹ä»¶2è¯¦æƒ…**")
            st.write(f"æ—¥æœŸ: {event2['event_date']}")
            st.write(f"å¹³å°: {event2['platform']}")
            st.write(f"å¸–å­æ•°: {event2['post_count']}")
    
    with tab3:
        st.subheader("è´¦å·èµ„äº§åº“")
        
        if not cross_accounts.empty:
            st.dataframe(
                cross_accounts,
                use_container_width=True,
                hide_index=True,
                column_config={
                    "user_id": "ç”¨æˆ·ID",
                    "username": "ç”¨æˆ·å",
                    "platform": "å¹³å°",
                    "event_count": "å‚ä¸äº‹ä»¶æ•°"
                }
            )
            
            csv = cross_accounts.to_csv(index=False)
            st.download_button(
                label="ğŸ“¥ å¯¼å‡ºè´¦å·åˆ—è¡¨ (CSV)",
                data=csv,
                file_name=f"cross_event_accounts_{datetime.now().strftime('%Y%m%d')}.csv",
                mime="text/csv"
            )
        else:
            st.info("æš‚æ— è·¨äº‹ä»¶è´¦å·æ•°æ®")


def create_app() -> None:
    st.set_page_config(
        page_title="ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æå¹³å°",
        page_icon="ğŸ“Š",
        layout="wide",
        initial_sidebar_state="expanded",
        menu_items={
            "About": "ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æå¯è§†åŒ–å¹³å° - å¤šå¹³å°æ•°æ®é‡‡é›†ä¸åˆ†æ"
        }
    )
    
    st.markdown("""
        <style>
        .stMetric {
            background-color: #f8f9fa;
            padding: 15px;
            border-radius: 10px;
            border: 1px solid #dee2e6;
        }
        .stMetric label {
            font-size: 14px;
            color: #6c757d;
        }
        .stMetric value {
            font-size: 24px;
            font-weight: bold;
        }
        div[data-testid="stHorizontalBlock"] > div {
            gap: 10px;
        }
        </style>
    """, unsafe_allow_html=True)
    
    st.title("ğŸ“Š ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æå¹³å°")
    st.markdown("---")
    
    filter_config = render_sidebar()
    
    analysis_type = st.session_state.get("analysis_type_radio", "æ¦‚è§ˆ")
    
    try:
        if analysis_type == "æ¦‚è§ˆ":
            render_overview_page(filter_config)
        elif analysis_type == "è¶‹åŠ¿åˆ†æ":
            render_trend_analysis_page(filter_config)
        elif analysis_type == "ç½‘ç»œæ‹“æ‰‘":
            render_network_page(filter_config)
        elif analysis_type == "å¼‚å¸¸æ£€æµ‹":
            render_anomaly_page(filter_config)
        elif analysis_type == "è·¨äº‹ä»¶åˆ†æ":
            render_cross_event_page(filter_config)
    except Exception as e:
        st.error(f"é¡µé¢æ¸²æŸ“é”™è¯¯: {str(e)}")
        logger.error(f"Dashboard render error: {e}")
    
    st.markdown("---")
    st.caption(f"æœ€åæ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


def run_dashboard(host: str = "localhost", port: int = 8501) -> None:
    import subprocess
    import sys

    logger.info(f"Starting dashboard on {host}:{port}")
    subprocess.run([
        sys.executable, "-m", "streamlit", "run",
        __file__,
        "--server.address", host,
        "--server.port", str(port),
        "--browser.gatherUsageStats", "false"
    ])


if __name__ == "__main__":
    create_app()
